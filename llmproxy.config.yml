proxy_configuration:
  route_type: cost

optional_configuration:
  timeout: 10 # Timeout for request to models
  force_timeout: False # WARNING: This can cause additonal costs!

provider_settings:
 - provider: OpenAI
   api_key_var: OPENAI_API_KEY # .env name for api key
   max_output_tokens: 4096
   temperature: 0.1
   models: # input names of models you want to use (see below for all models provided)
     # Only models specified here will be added to routing pool
     - gpt-4o
     - gpt-4-turbo
     - gpt-3.5-turbo-0125
 
 - provider: Cohere
   api_key_var: COHERE_API_KEY
   max_output_tokens: 4000
   temperature: 0.1
   models:
     - command-r-plus
     - command-r
     - command-nightly

 # - provider: Mistral
 #   api_key_var: MISTRAL_API_KEY
 #   max_output_tokens: 4096
 #   temperature: 0.1
 #   models:
 #     - open-mistral-7b
 #     - open-mixtral-8x7b
 #     - mistral-small-latest 
 #     - mistral-medium-latest
 #     - mistral-large-latest

 # - provider: VertexAI
 #   temperature: 0.1
 #   project_id_var: GOOGLE_PROJECT_ID
 #   max_output_tokens: 4096
 #   models:
 #     # Chat not supported
 #     # - text-bison
 #     # - code-bison
 #     # - code-gecko # max_output_tokens needs to change in order to work (1 <= max_output_tokens <= 65)
 #     # - chat-bison 
 #     # - codechat-bison
 #     - gemini-pro

 - provider: Anthropic
   api_key_var: ANTHROPIC_API_KEY
   temperature: 0.1
   max_output_tokens: 4096
   models:
    - claude-3-opus-20240229
    - claude-3-sonnet-20240229
    - claude-3-haiku-20240307
